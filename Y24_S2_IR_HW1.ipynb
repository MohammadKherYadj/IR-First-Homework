{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMo1mViyyLBc"
      },
      "source": [
        "# وظيفة 1:\n",
        "\n",
        "Name1 : محمد خير راتب يدج\n",
        "\n",
        "ID1 : 021041854\n",
        "\n",
        "Name2 : علي محمد درويش\n",
        "\n",
        "ID2 : 020041662"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0D_9Tn5yIlR"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HvYURUu4w4GB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "#mport arabic_reshaper\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "import re\n",
        "import qalsadi.lemmatizer as lemmatizer\n",
        "from snowballstemmer import stemmer\n",
        "# from spellchecker import SpellChecker\n",
        "\n",
        "from ar_corrector.corrector import Corrector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJHJkxI2yyKT"
      },
      "source": [
        "## Prepare classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnYuMPSUy4Sy"
      },
      "source": [
        "### Preprcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE2V8L3ySmzr"
      },
      "source": [
        "#### Task-1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "90idV23hyxT2"
      },
      "outputs": [],
      "source": [
        "#-preprocessor\n",
        "class Preprocessor():\n",
        "    @staticmethod\n",
        "    def process(sentence):\n",
        "        # print(\"The Result is :\")            \n",
        "        s = Preprocessor.normDocument(sentence)\n",
        "        s = Preprocessor.removestopwords(s)\n",
        "        s = Preprocessor.stemDocument(s)\n",
        "        # s = Preprocessor.lemmatizDocument(s)\n",
        "        s = Preprocessor.correctDocument(s)\n",
        "        return Preprocessor.tokenizeDocument(s)\n",
        "    @staticmethod\n",
        "    def tokenizeDocument(sentance):\n",
        "        tokenizer = WhitespaceTokenizer()\n",
        "        return tokenizer.tokenize(sentance)\n",
        "    \n",
        "    @staticmethod\n",
        "    def removestopwords(sentence):\n",
        "        terms=[]\n",
        "        stopWords= set(stp.stopwords_list())\n",
        "        for term in Preprocessor.tokenizeDocument(sentence) :\n",
        "            if term not in stopWords :\n",
        "                terms.append(term)\n",
        "        terms = list(terms)\n",
        "        return \" \".join(terms)\n",
        "    \n",
        "    @staticmethod\n",
        "    def lemmatizDocument(sentence):\n",
        "        ar_lemmer = lemmatizer.Lemmatizer()\n",
        "        return \" \".join([ar_lemmer.lemmatize(i) for i in Preprocessor.tokenizeDocument(sentence)])     # type: ignore\n",
        "\n",
        "    @staticmethod\n",
        "    def stemDocument(sentence):\n",
        "        ar_stemmer = stemmer(\"arabic\")\n",
        "        return \" \".join([ar_stemmer.stemWord(i) for i in Preprocessor.tokenizeDocument(sentence)])    \n",
        "\n",
        "    @staticmethod\n",
        "    def normDocument(sentence):\n",
        "        text = re.sub(\"[إأٱآا]\", \"ا\", sentence)\n",
        "        text = re.sub(\"ى\", \"ي\", text)\n",
        "        # text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
        "        text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%'()*+,،-./:;<=>؟?@[\\]^_`{}\"\"\"), ' ', text)\n",
        "        text = re.sub(\"ؤ\", \"ء\", text)\n",
        "        text = re.sub(\"ئ\", \"ء\", text)\n",
        "        text = re.sub(\"ة\", \"ه\", text)\n",
        "        return(text)\n",
        "    \n",
        "    @staticmethod\n",
        "    def correctDocument(sentance):\n",
        "        # Test (1) \n",
        "        # corr = SpellChecker(language=\"ar\") \n",
        "        # return \" \".join([corr.correction(term) for term in Preprocessor.tokenizeDocument(sentance)])\n",
        "        corr = Corrector()        \n",
        "        return \" \".join([corr.contextual_correct(term) for term in Preprocessor.tokenizeDocument(sentance)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "d0 = \"برنامج تعليمي للغة الإنجليزية و مسار سريع\"\n",
        "d1 = \"تعلم الفهرسة الدلالية الكامنة\"\n",
        "d2 = \"الكتاب في الفهرسة الدلالية\"\n",
        "d3 = \"التقدم في البنية و الفهرسة الدلالية\"\n",
        "d4 = \"تحليل تحليل البنية الكامنة\"\n",
        "\n",
        "doc_list = []\n",
        "doc_list.append(d0)\n",
        "doc_list.append(d1)\n",
        "doc_list.append(d2)\n",
        "doc_list.append(d3)\n",
        "doc_list.append(d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['برنامج', 'تعليم', 'للغه', 'انجليزيه', 'مسار', 'سريع']\n",
            "['تعلم', 'فهرسه', 'دلالة', 'كامنه']\n",
            "['كتاب', 'فهرسه', 'دلالة']\n",
            "['تقدم', 'بنيه', 'فهرسه', 'دلالة']\n",
            "['تحليل', 'تحليل', 'بنيه', 'كامنه']\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "   print(Preprocessor.process(doc_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsgzfEAQzK-v"
      },
      "source": [
        "### TDIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsvCDB3MqvXB"
      },
      "source": [
        "#### Task-2:\n",
        "\n",
        "قم بالتعديل على باني الكلاس\n",
        "\n",
        "IndexModel\n",
        "\n",
        "بحيث يصبح نوع الفهرس المستخدم  من نمط معطيات جدول, أي\n",
        "\n",
        "data type of the attribute \"_index\" is DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQVYjKEIzNbp"
      },
      "outputs": [],
      "source": [
        "#- Index Model : Term Document Incidence Matrix\n",
        "class IndexModel:\n",
        "\n",
        "    def __init__(self, documents_df):\n",
        "        print(\"question-2.1\")\n",
        "        self._index = {}\n",
        "        termdoc = documents_df.to_dict('list')\n",
        "        # display(termdoc)\n",
        "        uniqterm = []\n",
        "        for terms in Preprocessor.process(termdoc['text'][0]):\n",
        "            uniqterm.append(terms)\n",
        "        # print(uniqterm)\n",
        "        uniqterm = set(uniqterm)\n",
        "        # print(uniqterm)\n",
        "        # print(Preprocessor.process(termdoc['text'][0])[0])\n",
        "\n",
        "        # for term2 in uniqterm:\n",
        "        #     if term2 == Preprocessor.process(termdoc['text'][0])[-1]:\n",
        "        #         print(term2)\n",
        "\n",
        "        for document in termdoc['text']:\n",
        "            # print(document)\n",
        "            docID = IndexModel.getdocumentID(uniqterm,document)\n",
        "            print(docID)\n",
        "\n",
        "\n",
        "       \n",
        "    @staticmethod\n",
        "    def getdocumentID(uniqterm,document):\n",
        "        idlist =[]\n",
        "        num = 0\n",
        "        for term2 in uniqterm:\n",
        "            if term2 == Preprocessor.process(document)[num]:    \n",
        "                    idlist.append()\n",
        "        num+=1        \n",
        "        print(Preprocessor.process(document)[0])               \n",
        "\n",
        "    @staticmethod\n",
        "    def getfrequncy(document,term):\n",
        "        j=0\n",
        "        document = Preprocessor.process(document)\n",
        "        for i in document:\n",
        "            if i == term:\n",
        "                j+=1\n",
        "        return j\n",
        "\n",
        "\n",
        "    def term_incidence_vector(self, term):\n",
        "        print(\"question-2.2\")\n",
        "        # اكتب اجابتك هنا\n",
        "        #return ...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IndexModel.getfrequncy(d1,\"سريع\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# { \n",
        "#   \"term1\" : [(d1,5), (d2,1), (d5,7)],\n",
        "#   \"term2\" : [(d3,3), (d4,6), (d5,2)],\n",
        "#   \"term3\" : [(d1,2), (d2,7), (d3,6)],\n",
        "#  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = IndexModel(documents_df = loadDocuments())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35spCMQ9rtRd"
      },
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNPN8TnvruIj"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Retriever:\n",
        "    def __init__(self):\n",
        "        self._terms_operator = ['&', '|', '~']\n",
        "\n",
        "    def _merge(self, l1, l2):\n",
        "        res = []\n",
        "        i1 = 0; i2 = 0\n",
        "        while i1 < len(l1) and i2 < len(l2):\n",
        "            if l1[i1]==l2[i2]:\n",
        "                res += [l1[i1]]\n",
        "                i1 += 1; i2 += 1\n",
        "            elif l1[i1]<l2[i2]:\n",
        "                i1 += 1\n",
        "            elif l2[i2]<l1[i1]:\n",
        "                i2 += 1\n",
        "        return res\n",
        "\n",
        "    def boolean_operator_processing(self, bop, prevS, nextS=None, docidlist= None):\n",
        "        if bop == \"&\":\n",
        "            return self._merge(prevS, nextS)\n",
        "        elif bop==\"|\" :\n",
        "            return list(set(prevS+nextS))\n",
        "        elif bop == \"~\":\n",
        "            return [a for a in docidlist if a not in prevS]\n",
        "\n",
        "    def retrieve(self, query_terms, index_model):\n",
        "        ret_docs = []\n",
        "        bitwiseop=\"\"\n",
        "        result=[]\n",
        "        has_previous_term=False\n",
        "        has_not_operation=False\n",
        "        inc_vec_prev=[]\n",
        "        inc_vec_next=[]\n",
        "        for term in query_terms:\n",
        "            if term not in self._terms_operator:\n",
        "                if has_not_operation:\n",
        "                    if has_previous_term:\n",
        "                        inc_vec_next=self.boolean_operator_processing(bop=\"~\",prevS=index_model.term_postings(term),nextS=inc_vec_next, docidlist=index_model.getdi())\n",
        "                    else :\n",
        "                        inc_vec_prev=self.boolean_operator_processing(bop=\"~\",prevS=index_model.term_postings(term),nextS=inc_vec_next, docidlist=index_model.getdi())\n",
        "                        result=inc_vec_prev\n",
        "                    has_not_operation=False\n",
        "                elif has_previous_term:\n",
        "                    inc_vec_next=index_model.term_postings(term)\n",
        "                else:\n",
        "                    inc_vec_prev=index_model.term_postings(term)\n",
        "                    result= inc_vec_prev\n",
        "                    has_previous_term=True\n",
        "            elif term ==\"~\":\n",
        "                has_not_operation=True\n",
        "            else:\n",
        "                bitwiseop=term\n",
        "\n",
        "            #----------\n",
        "            if len(inc_vec_next)!= 0  :\n",
        "                result = self.boolean_operator_processing(bop=bitwiseop,prevS=inc_vec_prev,nextS=inc_vec_next, docidlist=index_model.getdi())\n",
        "                inc_vec_prev=result\n",
        "                has_previous_term=True\n",
        "                inc_vec_next= []\n",
        "\n",
        "        #-----\n",
        "        for res in result:\n",
        "            ret_docs.append({'docno':res, 'score':1})\n",
        "        ret_docs = pd.DataFrame(ret_docs, columns=['docno', 'score', 'content']).sort_values(by=['score'], ascending=False)\n",
        "        return ret_docs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mi2vBaQsAt2"
      },
      "source": [
        "### search engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfCwzu5xr-oI"
      },
      "outputs": [],
      "source": [
        "class SearchEngine:\n",
        "    def __init__(self, preprocessor, retriever, documents):\n",
        "        self.preprocessor = preprocessor\n",
        "        self.retriever = retriever\n",
        "        self.documents = None\n",
        "        self.model = None\n",
        "        self.rebuild(documents)\n",
        "\n",
        "    def rebuild(self, documents):\n",
        "        self.documents = documents\n",
        "        self.documents['ntext'] = self.documents['text'].apply(self.preprocessor.process)\n",
        "        self.model = IndexModel(self.documents)\n",
        "\n",
        "    def querying(self, query):\n",
        "        query_terms = self.preprocessor.process(query)\n",
        "        docs_res = self.retriever.retrieve(query_terms, self.model)\n",
        "        if docs_res.shape[0]>0:\n",
        "            docs_res['content'] = docs_res.apply(\n",
        "                lambda row: self.documents[self.documents['id']==row.id]['text'].iloc[0], axis = 1)\n",
        "        return docs_res\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgJag5izsMVM"
      },
      "source": [
        "## Build and use the IRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PQ7fwXsY1Y"
      },
      "source": [
        "#### Task-3:\n",
        "\n",
        "انشئ جدول وثائق من مجموعة العبارات التالية\n",
        "\n",
        "\n",
        "1. برنامج تعليمي للغة الإنجليزية ومسار سريع\n",
        "2. تعلم الفهرسة الدلالية الكامنة\n",
        "3. الكتاب في الفهرسة الدلالية\n",
        "4. التقدم في البنية والفهرسة الدلالية\n",
        "5. تحليل تحليل البنية الكامنة\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FodJ81IxsN2m"
      },
      "outputs": [],
      "source": [
        "d1 = \"برنامج تعليمي للغة الإنجليزية ومسار سريع\"\n",
        "d2 = \"تعلم الفهرسة الدلالية الكامنة\"\n",
        "d3 = \"الكتاب في الفهرسة الدلالية\"\n",
        "d4 = \"التقدم في البنية والفهرسة الدلالية\"\n",
        "d5 = \"تحليل تحليل البنية الكامنة\"\n",
        "\n",
        "def loadDocuments():\n",
        "    return pd.DataFrame([\n",
        "        [0,d1],\n",
        "        [1,d2],\n",
        "        [2,d3],\n",
        "        [3,d4],\n",
        "        [4,d5],\n",
        "    ],columns=[\"id\",\"text\"])\n",
        "df = loadDocuments()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sobZu31tbvj"
      },
      "source": [
        "#### Task-4:\n",
        "\n",
        "انشئ كيان من نظام محرك البحث"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npwIX3PWtktw"
      },
      "outputs": [],
      "source": [
        "# اكتب اجابتك هنا\n",
        "#ir_sys = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIffiofYtwT8"
      },
      "source": [
        "#### Task-5:\n",
        "\n",
        "اعرض اخر 3 وثائق\n",
        "\n",
        "واعرض حجم المصفوفة TDIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnR2LCeGt4lv"
      },
      "outputs": [],
      "source": [
        "# اخر 3 وثائق\n",
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiwIldSqM5yP"
      },
      "outputs": [],
      "source": [
        "# حجم مصفوفة وجود الكلمات في الوثائق TDIM\n",
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK-nT25quEZm"
      },
      "source": [
        "#### Task-6:\n",
        "\n",
        "قم بعرض الوثائق الموافقة للاستعلامات التالية"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Ix1cqhlusP"
      },
      "source": [
        "Query-1 = \"التقدم & البنية & ~ تحليل\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhoCpEB9uIRW"
      },
      "outputs": [],
      "source": [
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwIIbQv5lTsL"
      },
      "source": [
        "Query-2 = \"الكامنة & ~ تعليمي\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfvLMB4rlbMJ"
      },
      "outputs": [],
      "source": [
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX5Y5guBlbpm"
      },
      "source": [
        "Query-3 = \"التقدم والبنية بدون تحليل\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iuc5FPplcVX"
      },
      "outputs": [],
      "source": [
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccskZGjclc0K"
      },
      "source": [
        "Query-4 = \"تعلم | تحليل & الكامنة\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UW0KIToldMR"
      },
      "outputs": [],
      "source": [
        "# اكتب اجابتك هنا\n",
        "#..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
