{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "import re\n",
    "import qalsadi.lemmatizer as lemmatizer\n",
    "from snowballstemmer import stemmer\n",
    "from ar_corrector.corrector import Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-preprocessor\n",
    "class Preprocessor():\n",
    "    @staticmethod\n",
    "    def process(sentence):\n",
    "        # print(\"The Result is :\")            \n",
    "        s = Preprocessor.normDocument(sentence)\n",
    "        s = Preprocessor.removestopwords(s)\n",
    "        s = Preprocessor.stemDocument(s)\n",
    "        # s = Preprocessor.lemmatizDocument(s)\n",
    "        s = Preprocessor.correctDocument(s)\n",
    "        return Preprocessor.tokenizeDocument(s)\n",
    "    @staticmethod\n",
    "    def tokenizeDocument(sentance):\n",
    "        tokenizer = WhitespaceTokenizer()\n",
    "        return tokenizer.tokenize(sentance)\n",
    "    \n",
    "    @staticmethod\n",
    "    def removestopwords(sentence):\n",
    "        terms=[]\n",
    "        stopWords= set(stp.stopwords_list())\n",
    "        for term in Preprocessor.tokenizeDocument(sentence) :\n",
    "            if term not in stopWords :\n",
    "                terms.append(term)\n",
    "        terms = list(terms)\n",
    "        return \" \".join(terms)\n",
    "    \n",
    "    @staticmethod\n",
    "    def lemmatizDocument(sentence):\n",
    "        ar_lemmer = lemmatizer.Lemmatizer()\n",
    "        return \" \".join([ar_lemmer.lemmatize(i) for i in Preprocessor.tokenizeDocument(sentence)])     # type: ignore\n",
    "\n",
    "    @staticmethod\n",
    "    def stemDocument(sentence):\n",
    "        ar_stemmer = stemmer(\"arabic\")\n",
    "        return \" \".join([ar_stemmer.stemWord(i) for i in Preprocessor.tokenizeDocument(sentence)])    \n",
    "\n",
    "    @staticmethod\n",
    "    def normDocument(sentence):\n",
    "        text = re.sub(\"[إأٱآا]\", \"ا\", sentence)\n",
    "        text = re.sub(\"ى\", \"ي\", text)\n",
    "        text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "        text = re.sub(\"ؤ\", \"ء\", text)\n",
    "        text = re.sub(\"ئ\", \"ء\", text)\n",
    "        text = re.sub(\"ة\", \"ه\", text)\n",
    "        return(text)\n",
    "    \n",
    "    @staticmethod\n",
    "    def correctDocument(sentance):    \n",
    "        corr = Corrector()        \n",
    "        return \" \".join([corr.contextual_correct(term) for term in Preprocessor.tokenizeDocument(sentance)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>برنامج تعليمي للغة الإنجليزية و مسار سريع</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>تعلم الفهرسة الدلالية الكامنة</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>الكتاب في الفهرسة الدلالية</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>التقدم في البنية و الفهرسة الدلالية</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>تحليل تحليل البنية الكامنة</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       text\n",
       "0   0  برنامج تعليمي للغة الإنجليزية و مسار سريع\n",
       "1   1              تعلم الفهرسة الدلالية الكامنة\n",
       "2   2                 الكتاب في الفهرسة الدلالية\n",
       "3   3        التقدم في البنية و الفهرسة الدلالية\n",
       "4   4                 تحليل تحليل البنية الكامنة"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"برنامج تعليمي للغة الإنجليزية و مسار سريع\"\n",
    "d2 = \"تعلم الفهرسة الدلالية الكامنة\"\n",
    "d3 = \"الكتاب في الفهرسة الدلالية\"\n",
    "d4 = \"التقدم في البنية و الفهرسة الدلالية\"\n",
    "d5 = \"تحليل تحليل البنية الكامنة\"\n",
    "\n",
    "def loadDocuments():\n",
    "    return pd.DataFrame([\n",
    "        [0,d1],\n",
    "        [1,d2],\n",
    "        [2,d3],\n",
    "        [3,d4],\n",
    "        [4,d5],\n",
    "    ],columns=[\"id\",\"text\"])\n",
    "df = loadDocuments()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "for i in range(df[\"id\"].size):\n",
    "   list_data.append(Preprocessor.process(df[\"text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['برنامج', 'تعليم', 'للغه', 'انجليزيه', 'مسار', 'سريع'],\n",
       " ['تعلم', 'فهرسه', 'دلالة', 'كامنه'],\n",
       " ['كتاب', 'فهرسه', 'دلالة'],\n",
       " ['تقدم', 'بنيه', 'فهرسه', 'دلالة'],\n",
       " ['تحليل', 'تحليل', 'بنيه', 'كامنه']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['برنامج', 'تعليم', 'للغه', 'انجليزيه', 'مسار', 'سريع'],\n",
       " 1: ['تعلم', 'فهرسه', 'دلالة', 'كامنه'],\n",
       " 2: ['كتاب', 'فهرسه', 'دلالة'],\n",
       " 3: ['تقدم', 'بنيه', 'فهرسه', 'دلالة'],\n",
       " 4: ['تحليل', 'تحليل', 'بنيه', 'كامنه']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {0: ['برنامج', 'تعليم', 'للغه', 'انجليزيه', 'مسار', 'سريع'],\n",
    " 1: ['تعلم', 'فهرسه', 'دلالة', 'كامنه'],\n",
    " 2: ['كتاب', 'فهرسه', 'دلالة'],\n",
    " 3: ['تقدم', 'بنيه', 'فهرسه', 'دلالة'],\n",
    " 4: ['تحليل', 'تحليل', 'بنيه', 'كامنه']}\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(2,\"ntext\",list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ntext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>برنامج تعليمي للغة الإنجليزية و مسار سريع</td>\n",
       "      <td>[برنامج, تعليم, للغه, انجليزيه, مسار, سريع]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>تعلم الفهرسة الدلالية الكامنة</td>\n",
       "      <td>[تعلم, فهرسه, دلالة, كامنه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>الكتاب في الفهرسة الدلالية</td>\n",
       "      <td>[كتاب, فهرسه, دلالة]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>التقدم في البنية و الفهرسة الدلالية</td>\n",
       "      <td>[تقدم, بنيه, فهرسه, دلالة]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>تحليل تحليل البنية الكامنة</td>\n",
       "      <td>[تحليل, تحليل, بنيه, كامنه]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       text  \\\n",
       "0   0  برنامج تعليمي للغة الإنجليزية و مسار سريع   \n",
       "1   1              تعلم الفهرسة الدلالية الكامنة   \n",
       "2   2                 الكتاب في الفهرسة الدلالية   \n",
       "3   3        التقدم في البنية و الفهرسة الدلالية   \n",
       "4   4                 تحليل تحليل البنية الكامنة   \n",
       "\n",
       "                                         ntext  \n",
       "0  [برنامج, تعليم, للغه, انجليزيه, مسار, سريع]  \n",
       "1                  [تعلم, فهرسه, دلالة, كامنه]  \n",
       "2                         [كتاب, فهرسه, دلالة]  \n",
       "3                   [تقدم, بنيه, فهرسه, دلالة]  \n",
       "4                  [تحليل, تحليل, بنيه, كامنه]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تعلم',\n",
       " 'فهرسه',\n",
       " 'بنيه',\n",
       " 'تحليل',\n",
       " 'تقدم',\n",
       " 'مسار',\n",
       " 'انجليزيه',\n",
       " 'للغه',\n",
       " 'سريع',\n",
       " 'دلالة',\n",
       " 'برنامج',\n",
       " 'كامنه',\n",
       " 'تعليم',\n",
       " 'كتاب']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqterm = []\n",
    "i = 0\n",
    "for i in range(len(dic)):\n",
    "    for terms in dic[i]:\n",
    "        uniqterm.append(terms)\n",
    "        i += 1\n",
    "uniqterm = list(set(uniqterm))\n",
    "uniqterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'مسار': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'انجليزيه': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'للغه': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'سريع': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'برنامج': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'تعليم': [(0, 6), (0, 6), (0, 6), (0, 6), (0, 6), (0, 6)],\n",
       " 'تعلم': [(1, 4), (1, 4), (1, 4), (1, 4)],\n",
       " 'فهرسه': [(3, 4), (3, 4), (3, 4), (3, 4)],\n",
       " 'دلالة': [(3, 4), (3, 4), (3, 4), (3, 4)],\n",
       " 'كامنه': [(4, 4), (4, 4), (4, 4)],\n",
       " 'كتاب': [(2, 3), (2, 3), (2, 3)],\n",
       " 'بنيه': [(4, 4), (4, 4), (4, 4)],\n",
       " 'تقدم': [(3, 4), (3, 4), (3, 4), (3, 4)],\n",
       " 'تحليل': [(4, 4), (4, 4), (4, 4)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data ={}\n",
    "termdoc = df.to_dict('list')\n",
    "# for term1 in uniqterm:\n",
    "#     temp = []\n",
    "count = 0\n",
    "for i, c in enumerate(termdoc['ntext']):\n",
    "    # print(f\"{i}    {c}\")\n",
    "    termList = []\n",
    "    for uniq in uniqterm:\n",
    "        for g in range(len(c)):\n",
    "            if uniq in c:\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            pass\n",
    "        else:\n",
    "            termList.append((i,count))\n",
    "            final_data[uniq] = termList  \n",
    "            count = 0  \n",
    "\n",
    "final_data        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
